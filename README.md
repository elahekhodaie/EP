# Fictitious-Mean-field-Q-learning
This simulator implements the "Fictitious Mean-field Reinforcement-Learning" for Q-learning, Actor Critic and Stochastic Gradient Ascent algorithms in a distributed system. The setting includes strategic agents who compete over a set of heterogenous servers. Through rigorous experiments, we show that this algorithm outperforms the naive deployment of the single-agent version of each RL algorithm.
The environment variables can be found in the config text file.
discreteRun is the implementation of the original distributed system and test runs are becnhmarks.
